{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒê·ªÄ T√ÄI 9: PH√ÇN LO·∫†I N·∫§M (ƒÇN ƒê∆Ø·ª¢C HAY ƒê·ªòC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 1: Kh√°m ph√° v√† Ph√¢n t√≠ch D·ªØ li·ªáu (EDA)\n",
    "* M·ª•c ti√™u:\n",
    "  1. Hi·ªÉu c·∫•u tr√∫c v√† ƒë·∫∑c ƒëi·ªÉm c·ªßa dataset (s·ªë m·∫´u, s·ªë ƒë·∫∑c tr∆∞ng, ki·ªÉu d·ªØ li·ªáu).\n",
    "  2. Ph√¢n t√≠ch ph√¢n ph·ªëi c·ªßa bi·∫øn m·ª•c ti√™u ('class') ƒë·ªÉ ƒë√°nh gi√° m·∫•t c√¢n b·∫±ng l·ªõp (class imbalance).\n",
    "  3. Kh√°m ph√° c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i (categorical) th√¥ng qua t·∫ßn su·∫•t v√† bi·ªÉu ƒë·ªì ƒë·ªÉ nh·∫≠n di·ªán c√°c ƒë·∫∑c tr∆∞ng quan tr·ªçng.\n",
    "  4. Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu (missing values) v√† m·ªëi quan h·ªá gi·ªØa c√°c ƒë·∫∑c tr∆∞ng v·ªõi bi·∫øn m·ª•c ti√™u ƒë·ªÉ x√°c ƒë·ªãnh c√°c v·∫•n ƒë·ªÅ c·∫ßn x·ª≠ l√Ω trong ti·ªÅn x·ª≠ l√Ω.\n",
    "  5. Cung c·∫•p c∆° s·ªü cho c√°c b∆∞·ªõc ti·∫øp theo (preprocessing, feature engineering) b·∫±ng c√°ch x√°c ƒë·ªãnh c√°c ƒë·∫∑c tr∆∞ng c√≥ gi√° tr·ªã d·ª± ƒëo√°n cao v√† v·∫•n ƒë·ªÅ d·ªØ li·ªáu (nh∆∞ missing values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-09-22T15:10:55.766161Z",
     "shell.execute_reply": "2025-09-22T15:10:55.765384Z",
     "shell.execute_reply.started": "2025-09-22T15:10:44.117561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Thi·∫øt l·∫≠p style cho bi·ªÉu ƒë·ªì\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# 1. Load d·ªØ li·ªáu\n",
    "train_df = pd.read_csv('Dataset\\\\train.csv')\n",
    "test_df = pd.read_csv('Dataset\\\\test.csv')\n",
    "\n",
    "# 2. Ki·ªÉm tra c·∫•u tr√∫c d·ªØ li·ªáu\n",
    "print(\"=== C·∫•u tr√∫c Train Dataset ===\")\n",
    "print(f\"S·ªë m·∫´u: {train_df.shape[0]}\")\n",
    "print(f\"S·ªë ƒë·∫∑c tr∆∞ng (bao g·ªìm target v√† id): {train_df.shape[1]}\")\n",
    "print(\"\\nKi·ªÉu d·ªØ li·ªáu c√°c c·ªôt:\")\n",
    "print(train_df.dtypes)\n",
    "print(\"\\nTh√¥ng tin t·ªïng quan:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\n=== C·∫•u tr√∫c Test Dataset ===\")\n",
    "print(f\"S·ªë m·∫´u: {test_df.shape[0]}\")\n",
    "print(f\"S·ªë ƒë·∫∑c tr∆∞ng (bao g·ªìm target v√† id): {test_df.shape[1]}\")\n",
    "print(\"\\nKi·ªÉu d·ªØ li·ªáu c√°c c·ªôt:\")\n",
    "print(test_df.dtypes)\n",
    "\n",
    "# 3. Ph√¢n t√≠ch bi·∫øn m·ª•c ti√™u (class)\n",
    "print(\"\\n=== Ph√¢n ph·ªëi bi·∫øn m·ª•c ti√™u (class) trong Train ===\")\n",
    "print(train_df['class'].value_counts(normalize=True))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='class', data=train_df)\n",
    "plt.title('Ph√¢n ph·ªëi Class (Poisonous vs Edible) - Train')\n",
    "plt.xlabel('Class (e: Edible, p: Poisonous)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng\n",
    "# Ch√∫ th√≠ch: Kh√¥ng c√≥ ƒë·∫∑c tr∆∞ng s·ªë (numerical), ch·ªâ c√≥ categorical\n",
    "print(\"\\n=== Ch√∫ th√≠ch ===\")\n",
    "print(\"Dataset kh√¥ng c√≥ ƒë·∫∑c tr∆∞ng s·ªë, do ƒë√≥ kh√¥ng th·ª±c hi·ªán th·ªëng k√™ m√¥ t·∫£, histogram ho·∫∑c boxplot.\")\n",
    "\n",
    "# Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i\n",
    "categorical_cols = train_df.drop(columns=['class', 'id']).columns\n",
    "n_cols = 3  # S·ªë c·ªôt trong l∆∞·ªõi subplot\n",
    "n_rows = int(np.ceil(len(categorical_cols) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*6, n_rows*5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    sns.countplot(x=col, hue='class', data=train_df, ax=axes[i])\n",
    "    axes[i].set_title(f'Ph√¢n ph·ªëi {col} theo Class')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].legend(title='Class', loc='upper right')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# X√≥a c√°c subplot tr·ªëng\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('categorical_features_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 5. Ph√¢n t√≠ch d·ªØ li·ªáu thi·∫øu (missing)\n",
    "print(\"\\n=== Ph√¢n t√≠ch d·ªØ li·ªáu thi·∫øu (Train) ===\")\n",
    "missing_train = train_df.replace('?', np.nan).isnull().sum()\n",
    "print(missing_train[missing_train > 0])\n",
    "\n",
    "print(\"\\n=== Ph√¢n t√≠ch d·ªØ li·ªáu thi·∫øu (Test) ===\")\n",
    "missing_test = test_df.replace('?', np.nan).isnull().sum()\n",
    "print(missing_test[missing_test > 0])\n",
    "\n",
    "# Ph√¢n t√≠ch missing theo class\n",
    "print(\"\\n=== Ph√¢n t√≠ch Missing theo Class (Train) ===\")\n",
    "missing_by_class = train_df[train_df['stalk-root'] == '?'].groupby('class').size()\n",
    "print(missing_by_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='stalk-root', hue='class', data=train_df[train_df['stalk-root'] == '?'])\n",
    "plt.title('Ph√¢n ph·ªëi Missing Values (stalk-root) theo Class')\n",
    "plt.xlabel('stalk-root (Missing = ?)')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('missing_stalk_root_by_class.png')\n",
    "plt.show()\n",
    "\n",
    "# 6. Ph√¢n t√≠ch m·ªëi quan h·ªá: Chi-square test thay cho ma tr·∫≠n t∆∞∆°ng quan\n",
    "print(\"\\n=== Ph√¢n t√≠ch m·ªëi quan h·ªá (Chi-square test) ===\")\n",
    "chi2_results = {}\n",
    "for col in categorical_cols:\n",
    "    contingency_table = pd.crosstab(train_df[col], train_df['class'])\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    chi2_results[col] = p\n",
    "\n",
    "# Chuy·ªÉn k·∫øt qu·∫£ chi-square th√†nh DataFrame\n",
    "chi2_df = pd.DataFrame.from_dict(chi2_results, orient='index', columns=['p-value'])\n",
    "chi2_df = chi2_df.sort_values(by='p-value')\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"\\nP-values t·ª´ Chi-square test (th·∫•p h∆°n = ƒë·∫∑c tr∆∞ng quan tr·ªçng h∆°n):\")\n",
    "print(chi2_df)\n",
    "\n",
    "# V·∫Ω heatmap cho p-values\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(chi2_df, annot=True, cmap='coolwarm', fmt='.2e')\n",
    "plt.title('P-values c·ªßa Chi-square Test gi·ªØa ƒê·∫∑c tr∆∞ng v√† Class')\n",
    "plt.savefig('chi2_pvalues_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# 7. Ph√¢n t√≠ch gi√° tr·ªã unique c·ªßa c√°c ƒë·∫∑c tr∆∞ng\n",
    "print(\"\\n=== S·ªë gi√° tr·ªã unique c·ªßa t·ª´ng ƒë·∫∑c tr∆∞ng (Train) ===\")\n",
    "unique_counts = train_df.drop(columns=['id']).nunique()\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 2: Ti·ªÅn x·ª≠ l√Ω v√† K·ªπ thu·∫≠t ƒê·∫∑c tr∆∞ng (song song 2 lu·ªìng)\n",
    "* M·ª•c ti√™u chung:\n",
    "  1. Chu·∫©n b·ªã d·ªØ li·ªáu s·∫°ch v√† ph√π h·ª£p cho hu·∫•n luy·ªán m√¥ h√¨nh h·ªçc m√°y, ƒë·∫£m b·∫£o x·ª≠ l√Ω c√°c v·∫•n ƒë·ªÅ c·ªßa dataset (missing values, categorical data) v√† t·ªëi ∆∞u h√≥a ƒë·∫∑c tr∆∞ng ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh.\n",
    "  2. T·∫°o hai b·ªô d·ªØ li·ªáu ƒë·∫∑c tr∆∞ng ri√™ng bi·ªát t·ª´ hai lu·ªìng:\n",
    "     * Lu·ªìng A: S·ª≠ d·ª•ng k·ªπ thu·∫≠t truy·ªÅn th·ªëng ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu v√† t·∫°o ƒë·∫∑c tr∆∞ng th·ªß c√¥ng.\n",
    "     * Lu·ªìng B: S·ª≠ d·ª•ng autoencoder ƒë·ªÉ t·ª± ƒë·ªông tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu, t·∫°o b·ªô ƒë·∫∑c tr∆∞ng m·ªõi t·ª´ l·ªõp bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lu·ªìng A: K·ªπ thu·∫≠t Truy·ªÅn th·ªëng\n",
    "* M·ª•c ti√™u:\n",
    "  1. X·ª≠ l√Ω missing values: Thay '?' trong stalk-root b·∫±ng mode.\n",
    "  2. M√£ h√≥a categorical: S·ª≠ d·ª•ng c·∫£ One-Hot Encoding (phi√™n b·∫£n ch√≠nh) v√† Label Encoding (phi√™n b·∫£n b·ªï sung) ƒë·ªÉ t·∫°o hai b·ªô d·ªØ li·ªáu.\n",
    "  3. Chu·∫©n h√≥a/scale d·ªØ li·ªáu s·ªë: B·ªè qua (dataset kh√¥ng c√≥ d·ªØ li·ªáu s·ªë), th√™m ch√∫ th√≠ch r√µ r√†ng.\n",
    "  4. Feature engineering: T·∫°o th√™m ƒë·∫∑c tr∆∞ng (k·∫øt h·ª£p gill-color v√† spore-print-color, nh·ªã ph√¢n h√≥a bruises).\n",
    "  5. Feature selection: Lo·∫°i b·ªè ƒë·∫∑c tr∆∞ng √≠t quan tr·ªçng d·ª±a tr√™n chi-square p-value t·ª´ EDA.\n",
    "  6. K·∫øt qu·∫£: T·∫°o hai b·ªô d·ªØ li·ªáu:\n",
    "     * One-Hot: X_train_onehot.csv, X_test_onehot.csv, y_train_onehot.csv.\n",
    "     * Label Encoding: X_train_label.csv, X_test_label.csv, y_train_label.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T15:11:22.750482Z",
     "iopub.status.busy": "2025-09-22T15:11:22.749908Z",
     "iopub.status.idle": "2025-09-22T15:11:23.731960Z",
     "shell.execute_reply": "2025-09-22T15:11:23.731245Z",
     "shell.execute_reply.started": "2025-09-22T15:11:22.750454Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chi2_contingency\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Ki·ªÉm tra c·ªôt trong test_df\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mC·ªôt trong test_df:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[43mtest_df\u001b[49m.columns))\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 1. X·ª≠ l√Ω missing values\u001b[39;00m\n\u001b[32m     10\u001b[39m train_df[\u001b[33m'\u001b[39m\u001b[33mstalk-root\u001b[39m\u001b[33m'\u001b[39m] = train_df[\u001b[33m'\u001b[39m\u001b[33mstalk-root\u001b[39m\u001b[33m'\u001b[39m].replace(\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m, train_df[\u001b[33m'\u001b[39m\u001b[33mstalk-root\u001b[39m\u001b[33m'\u001b[39m].mode()[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Ki·ªÉm tra c·ªôt trong test_df\n",
    "print(\"C·ªôt trong test_df:\", list(test_df.columns))\n",
    "\n",
    "# 1. X·ª≠ l√Ω missing values\n",
    "train_df['stalk-root'] = train_df['stalk-root'].replace('?', train_df['stalk-root'].mode()[0])\n",
    "test_df['stalk-root'] = test_df['stalk-root'].replace('?', test_df['stalk-root'].mode()[0])\n",
    "\n",
    "# 2. Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt\n",
    "columns_to_drop = ['veil-type', 'veil-color', 'id']\n",
    "columns_to_drop = [col for col in columns_to_drop if col in train_df.columns]\n",
    "train_df = train_df.drop(columns=columns_to_drop)\n",
    "test_df = test_df.drop(columns=[col for col in columns_to_drop if col in test_df.columns])\n",
    "\n",
    "# 3. T√°ch features v√† target\n",
    "X_train = train_df.drop('class', axis=1)\n",
    "y_train = train_df['class']\n",
    "X_test = test_df  # test_df kh√¥ng c√≥ c·ªôt 'class'\n",
    "\n",
    "# Ch√∫ th√≠ch: Kh√¥ng c√≥ ƒë·∫∑c tr∆∞ng s·ªë n√™n b·ªè qua chu·∫©n h√≥a/scale\n",
    "print(\"Ch√∫ th√≠ch: Dataset kh√¥ng c√≥ ƒë·∫∑c tr∆∞ng s·ªë, b·ªè qua b∆∞·ªõc chu·∫©n h√≥a/scale (StandardScaler, MinMaxScaler).\")\n",
    "\n",
    "# 4. Feature selection d·ª±a tr√™n chi-square test\n",
    "chi2_results = {}\n",
    "for col in X_train.columns:\n",
    "    contingency_table = pd.crosstab(X_train[col], y_train)\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    chi2_results[col] = p\n",
    "\n",
    "# Ch·ªçn ƒë·∫∑c tr∆∞ng c√≥ p-value < 0.05 (quan tr·ªçng)\n",
    "important_features = [col for col, p in chi2_results.items() if p < 0.05]\n",
    "print(\"ƒê·∫∑c tr∆∞ng quan tr·ªçng (p-value < 0.05):\", important_features)\n",
    "\n",
    "# L·ªçc X_train v√† X_test ch·ªâ gi·ªØ ƒë·∫∑c tr∆∞ng quan tr·ªçng\n",
    "X_train_onehot = X_train[important_features]\n",
    "X_test_onehot = X_test[important_features]\n",
    "\n",
    "# 5. M√£ h√≥a categorical: One-Hot Encoding\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_train_onehot_encoded = ohe.fit_transform(X_train_onehot)\n",
    "X_test_onehot_encoded = ohe.transform(X_test_onehot)\n",
    "\n",
    "feature_names_onehot = ohe.get_feature_names_out(important_features)\n",
    "X_train_onehot_encoded = pd.DataFrame(X_train_onehot_encoded, columns=feature_names_onehot)\n",
    "X_test_onehot_encoded = pd.DataFrame(X_test_onehot_encoded, columns=feature_names_onehot)\n",
    "\n",
    "# 6. Feature engineering cho One-Hot\n",
    "# T·∫°o ƒë·∫∑c tr∆∞ng: odor + spore-print-color\n",
    "X_train_onehot_encoded['odor_spore_interaction'] = X_train['odor'] + '_' + X_train['spore-print-color']\n",
    "X_test_onehot_encoded['odor_spore_interaction'] = X_test['odor'] + '_' + X_test['spore-print-color']\n",
    "# T·∫°o ƒë·∫∑c tr∆∞ng: gill-color + spore-print-color\n",
    "X_train_onehot_encoded['gill_spore_interaction'] = X_train['gill-color'] + '_' + X_train['spore-print-color']\n",
    "X_test_onehot_encoded['gill_spore_interaction'] = X_test['gill-color'] + '_' + X_test['spore-print-color']\n",
    "# Nh·ªã ph√¢n h√≥a bruises\n",
    "X_train_onehot_encoded['bruises_binary'] = X_train['bruises'].map({'f': 0, 't': 1})\n",
    "X_test_onehot_encoded['bruises_binary'] = X_test['bruises'].map({'f': 0, 't': 1})\n",
    "\n",
    "# M√£ h√≥a c√°c ƒë·∫∑c tr∆∞ng m·ªõi\n",
    "ohe_interaction = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "interaction_train = ohe_interaction.fit_transform(X_train_onehot_encoded[['odor_spore_interaction', 'gill_spore_interaction']])\n",
    "interaction_test = ohe_interaction.transform(X_test_onehot_encoded[['odor_spore_interaction', 'gill_spore_interaction']])\n",
    "\n",
    "interaction_feature_names = ohe_interaction.get_feature_names_out(['odor_spore_interaction', 'gill_spore_interaction'])\n",
    "X_train_onehot_encoded = pd.concat([X_train_onehot_encoded.drop(['odor_spore_interaction', 'gill_spore_interaction'], axis=1),\n",
    "                                    pd.DataFrame(interaction_train, columns=interaction_feature_names)], axis=1)\n",
    "X_test_onehot_encoded = pd.concat([X_test_onehot_encoded.drop(['odor_spore_interaction', 'gill_spore_interaction'], axis=1),\n",
    "                                   pd.DataFrame(interaction_test, columns=interaction_feature_names)], axis=1)\n",
    "\n",
    "# 7. M√£ h√≥a categorical: Label Encoding\n",
    "X_train_label = X_train.copy()\n",
    "X_test_label = X_test.copy()\n",
    "label_encoders = {}\n",
    "for col in X_train_label.columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train_label[col] = le.fit_transform(X_train_label[col])\n",
    "    X_test_label[col] = le.transform(X_test_label[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Feature engineering cho Label Encoding\n",
    "X_train_label['odor_spore_interaction'] = X_train['odor'] + '_' + X_train['spore-print-color']\n",
    "X_test_label['odor_spore_interaction'] = X_test['odor'] + '_' + X_test['spore-print-color']\n",
    "X_train_label['gill_spore_interaction'] = X_train['gill-color'] + '_' + X_train['spore-print-color']\n",
    "X_test_label['gill_spore_interaction'] = X_test['gill-color'] + '_' + X_test['spore-print-color']\n",
    "X_train_label['bruises_binary'] = X_train['bruises'].map({'f': 0, 't': 1})\n",
    "X_test_label['bruises_binary'] = X_test['bruises'].map({'f': 0, 't': 1})\n",
    "\n",
    "# M√£ h√≥a c√°c ƒë·∫∑c tr∆∞ng m·ªõi\n",
    "le_interaction = LabelEncoder()\n",
    "X_train_label['odor_spore_interaction'] = le_interaction.fit_transform(X_train_label['odor_spore_interaction'])\n",
    "X_test_label['odor_spore_interaction'] = le_interaction.transform(X_test_label['odor_spore_interaction'])\n",
    "le_gill_spore = LabelEncoder()\n",
    "X_train_label['gill_spore_interaction'] = le_gill_spore.fit_transform(X_train_label['gill_spore_interaction'])\n",
    "X_test_label['gill_spore_interaction'] = le_gill_spore.transform(X_test_label['gill_spore_interaction'])\n",
    "\n",
    "# M√£ h√≥a target\n",
    "y_train = y_train.map({'e': 0, 'p': 1})\n",
    "\n",
    "# 8. L∆∞u d·ªØ li·ªáu\n",
    "X_train_onehot_encoded.to_csv('X_train_onehot.csv', index=False)\n",
    "X_test_onehot_encoded.to_csv('X_test_onehot.csv', index=False)\n",
    "X_train_label.to_csv('X_train_label.csv', index=False)\n",
    "X_test_label.to_csv('X_test_label.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "\n",
    "print(\"=== K·∫øt qu·∫£ Lu·ªìng A ===\")\n",
    "print(f\"X_train_onehot shape: {X_train_onehot_encoded.shape}\")\n",
    "print(f\"X_test_onehot shape: {X_test_onehot_encoded.shape}\")\n",
    "print(f\"X_train_label shape: {X_train_label.shape}\")\n",
    "print(f\"X_test_label shape: {X_test_label.shape}\")\n",
    "print(f\"One-Hot Feature names: {list(X_train_onehot_encoded.columns)}\")\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: X_train_onehot.csv, X_test_onehot.csv, X_train_label.csv, X_test_label.csv, y_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lu·ªìng B: Autoencoder\n",
    "* M·ª•c ti√™u:\n",
    "  1. X√¢y d·ª±ng ki·∫øn tr√∫c autoencoder: T·∫°o m√¥ h√¨nh encoder-bottleneck-decoder ƒë·ªÉ n√©n d·ªØ li·ªáu categorical ƒë√£ m√£ h√≥a th√†nh 32 chi·ªÅu.\n",
    "  2. Hu·∫•n luy·ªán autoencoder: D√πng h√†m m·∫•t m√°t MSE, optimizer Adam, tƒÉng s·ªë epochs l√™n 100, v√† th√™m early stopping ƒë·ªÉ tr√°nh overfitting.\n",
    "  3. Tr√≠ch xu·∫•t vector ƒë·∫∑c tr∆∞ng: L·∫•y output t·ª´ l·ªõp bottleneck l√†m b·ªô ƒë·∫∑c tr∆∞ng m·ªõi.\n",
    "  4. K·∫øt qu·∫£: T·∫°o X_train_autoencoder, X_test_autoencoder, y_train_autoencoder (kh√¥ng c√≥ y_test_autoencoder do test.csv thi·∫øu 'class')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T15:11:28.239352Z",
     "iopub.status.busy": "2025-09-22T15:11:28.238636Z",
     "iopub.status.idle": "2025-09-22T15:12:22.089855Z",
     "shell.execute_reply": "2025-09-22T15:12:22.089141Z",
     "shell.execute_reply.started": "2025-09-22T15:11:28.239309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout  # Th√™m Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ki·ªÉm tra c·ªôt trong test_df\n",
    "print(\"C·ªôt trong test_df:\", list(test_df.columns))\n",
    "\n",
    "# 1. X·ª≠ l√Ω missing values\n",
    "train_df['stalk-root'] = train_df['stalk-root'].replace('?', train_df['stalk-root'].mode()[0])\n",
    "test_df['stalk-root'] = test_df['stalk-root'].replace('?', test_df['stalk-root'].mode()[0])\n",
    "\n",
    "# 2. Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt\n",
    "columns_to_drop = ['veil-type', 'veil-color', 'id']\n",
    "columns_to_drop = [col for col in columns_to_drop if col in train_df.columns]\n",
    "train_df = train_df.drop(columns=columns_to_drop)\n",
    "test_df = test_df.drop(columns=[col for col in columns_to_drop if col in test_df.columns])\n",
    "\n",
    "# 3. T√°ch features v√† target\n",
    "X_train = train_df.drop('class', axis=1)\n",
    "y_train = train_df['class']\n",
    "X_test = test_df  # test_df kh√¥ng c√≥ c·ªôt 'class'\n",
    "\n",
    "# 4. M√£ h√≥a categorical features b·∫±ng One-Hot Encoding\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_train_encoded = ohe.fit_transform(X_train)\n",
    "X_test_encoded = ohe.transform(X_test)\n",
    "\n",
    "# L·∫•y t√™n c·ªôt\n",
    "feature_names = ohe.get_feature_names_out(X_train.columns)\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=feature_names)\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=feature_names)\n",
    "\n",
    "# M√£ h√≥a target\n",
    "y_train = y_train.map({'e': 0, 'p': 1})\n",
    "\n",
    "# 5. X√¢y d·ª±ng ki·∫øn tr√∫c autoencoder\n",
    "input_dim = X_train_encoded.shape[1]\n",
    "encoding_dim = 32  # C√≥ th·ªÉ tƒÉng l√™n 64 n·∫øu mu·ªën gi·ªØ th√™m th√¥ng tin\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "# encoded = Dropout(0.2)(encoded)  # Uncomment n·∫øu mu·ªën gi·∫£m overfitting\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)  # Bottleneck\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "encoder_model = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')  # C√≥ th·ªÉ gi·∫£m xu·ªëng 0.0001 n·∫øu loss dao ƒë·ªông\n",
    "\n",
    "# 6. Th√™m EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)  # C√≥ th·ªÉ tƒÉng patience=15 n·∫øu c·∫ßn\n",
    "\n",
    "# 7. Hu·∫•n luy·ªán autoencoder\n",
    "history = autoencoder.fit(X_train_encoded, X_train_encoded,\n",
    "                         epochs=100,  # C√≥ th·ªÉ tƒÉng l√™n 150 n·∫øu c·∫ßn th√™m th·ªùi gian h·ªçc\n",
    "                         batch_size=32,  # C√≥ th·ªÉ tƒÉng l√™n 64 n·∫øu c·∫ßn ·ªïn ƒë·ªãnh gradient\n",
    "                         validation_data=(X_test_encoded, X_test_encoded),\n",
    "                         callbacks=[early_stopping],\n",
    "                         verbose=1)\n",
    "\n",
    "# 8. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ bottleneck\n",
    "X_train_autoencoder = encoder_model.predict(X_train_encoded)\n",
    "X_test_autoencoder = encoder_model.predict(X_test_encoded)\n",
    "\n",
    "# Chuy·ªÉn th√†nh DataFrame\n",
    "X_train_autoencoder = pd.DataFrame(X_train_autoencoder, columns=[f'feature_{i}' for i in range(encoding_dim)])\n",
    "X_test_autoencoder = pd.DataFrame(X_test_autoencoder, columns=[f'feature_{i}' for i in range(encoding_dim)])\n",
    "\n",
    "# 9. L∆∞u d·ªØ li·ªáu\n",
    "X_train_autoencoder.to_csv('X_train_autoencoder.csv', index=False)\n",
    "X_test_autoencoder.to_csv('X_test_autoencoder.csv', index=False)\n",
    "y_train.to_csv('y_train_autoencoder.csv', index=False)\n",
    "\n",
    "# 10. V·∫Ω loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Autoencoder Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.savefig('autoencoder_loss_curve.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"=== K·∫øt qu·∫£ Lu·ªìng B ===\")\n",
    "print(f\"X_train_autoencoder shape: {X_train_autoencoder.shape}\")\n",
    "print(f\"X_test_autoencoder shape: {X_test_autoencoder.shape}\")\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: X_train_autoencoder.csv, X_test_autoencoder.csv, y_train_autoencoder.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 3: Hu·∫•n luy·ªán M√¥ h√¨nh\n",
    "* M·ª•c ti√™u:\n",
    "  1. √Åp d·ª•ng c√°c thu·∫≠t to√°n h·ªçc m√°y truy·ªÅn th·ªëng (Decision Tree, Random Forest, Logistic Regression, SVM) v√† m·ªü r·ªông sang ensemble (XGBoost, LightGBM) ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i nh·ªã ph√¢n (edible vs poisonous).\n",
    "  2. Train ri√™ng bi·ªát tr√™n d·ªØ li·ªáu t·ª´ Lu·ªìng A (One-Hot Encoding v√† Label Encoding) v√† Lu·ªìng B (Autoencoder features).\n",
    "  3. S·ª≠ d·ª•ng train/validation split (80/20) t·ª´ train set ƒë·ªÉ ƒë√°nh gi√°, v√¨ test set kh√¥ng c√≥ nh√£n (y_test).\n",
    "  4. T√≠nh c√°c ch·ªâ s·ªë ph√π h·ª£p cho ph√¢n lo·∫°i: Accuracy, Precision, Recall, F1, ROC-AUC.\n",
    "  5. L·∫≠p b·∫£ng so s√°nh hi·ªáu su·∫•t gi·ªØa c√°c m√¥ h√¨nh v√† c√°c lu·ªìng ƒë·ªÉ x√°c ƒë·ªãnh m√¥ h√¨nh/lu·ªìng t·ªët nh·∫•t.\n",
    "  6. D·ª± ƒëo√°n nh√£n cho test set v√† l∆∞u k·∫øt qu·∫£."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hu·∫•n luy·ªán cho Lu·ªìng A (One-Hot Encoding v√† Label Encoding)\n",
    "* M·ª•c ti√™u: Hu·∫•n luy·ªán v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh (Decision Tree, Random Forest, Logistic Regression, SVM, XGBoost, LightGBM) tr√™n d·ªØ li·ªáu t·ª´ Lu·ªìng A, bao g·ªìm c·∫£ One-Hot Encoding v√† Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T15:12:32.981650Z",
     "iopub.status.busy": "2025-09-22T15:12:32.980919Z",
     "iopub.status.idle": "2025-09-22T15:12:52.144387Z",
     "shell.execute_reply": "2025-09-22T15:12:52.143680Z",
     "shell.execute_reply.started": "2025-09-22T15:12:32.981625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================== 3A: Lu·ªìng A ‚Äì One-Hot & Label (gi·ªØ nguy√™n metrics + CM, s·ª≠a submission) =====================\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===================== I/O helpers =====================\n",
    "def smart_read_csv(candidates):\n",
    "    \"\"\"ƒê·ªçc CSV t·ª´ danh s√°ch ƒë∆∞·ªùng d·∫´n ·ª©ng vi√™n (tr·∫£ v·ªÅ df ƒë·∫ßu ti√™n ƒë·ªçc ƒë∆∞·ª£c).\"\"\"\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p):\n",
    "            try:\n",
    "                return pd.read_csv(p)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def get_test_ids():\n",
    "    test_df = smart_read_csv([\n",
    "        \"/kaggle/working/test.csv\",\n",
    "        \"./test.csv\",\n",
    "        \"/mnt/data/test.csv\",\n",
    "        \"/kaggle/input/mushroom-classification-btl/test.csv\",\n",
    "    ])\n",
    "    if test_df is not None:\n",
    "        if 'id' in test_df.columns:\n",
    "            return test_df['id']               # gi·ªØ nguy√™n ch·ªØ th∆∞·ªùng\n",
    "        if 'Id' in test_df.columns:\n",
    "            return test_df['Id'].rename('id')  # √©p v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "    return None\n",
    "\n",
    "# ===================== Chu·∫©n ho√° nh√£n cho ph·∫ßn ƒë√°nh gi√° =====================\n",
    "def _to_binary(y):\n",
    "    \"\"\"Chu·∫©n ho√° nh√£n v·ªÅ 0/1. H·ªó tr·ª£ y l√† 0/1 ho·∫∑c 'e'/'p'.\"\"\"\n",
    "    s = pd.Series(y)\n",
    "    uniq = set(s.unique())\n",
    "    if uniq <= {0, 1}:\n",
    "        return s.astype(int)\n",
    "    mapping = {'e': 0, 'p': 1, 'E': 0, 'P': 1}\n",
    "    out = s.map(mapping)\n",
    "    if out.isna().any():\n",
    "        raise ValueError(f\"Nh√£n kh√¥ng thu·ªôc {{0,1,'e','p'}}: {sorted(list(uniq))[:5]}\")\n",
    "    return out.astype(int)\n",
    "\n",
    "def _pos_index_for_proba(model):\n",
    "    \"\"\"L·∫•y index c·ªßa l·ªõp d∆∞∆°ng trong predict_proba theo model.classes_.\"\"\"\n",
    "    if hasattr(model, \"classes_\"):\n",
    "        classes = list(model.classes_)\n",
    "        if 'p' in classes:  # ∆∞u ti√™n l·ªõp 'p' n·∫øu h·ªçc tr·ª±c ti·∫øp e/p\n",
    "            return classes.index('p')\n",
    "        if 1 in classes:    # n·∫øu h·ªçc nh·ªã ph√¢n 0/1 th√¨ l·∫•y l·ªõp 1\n",
    "            return classes.index(1)\n",
    "        try:\n",
    "            return classes.index(max(classes))\n",
    "        except Exception:\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "# ===================== Load features/labels =====================\n",
    "X_train_onehot = pd.read_csv('/kaggle/working/X_train_onehot.csv')\n",
    "X_test_onehot  = pd.read_csv('/kaggle/working/X_test_onehot.csv')\n",
    "y_train        = pd.read_csv('/kaggle/working/y_train.csv')['class']  # c√≥ th·ªÉ l√† 'e'/'p' ho·∫∑c 0/1\n",
    "\n",
    "X_train_label  = pd.read_csv('/kaggle/working/X_train_label.csv')\n",
    "X_test_label   = pd.read_csv('/kaggle/working/X_test_label.csv')\n",
    "\n",
    "# Stratified split (d√πng nh√£n g·ªëc, kh√¥ng √©p s·ªõm ƒë·ªÉ tr√°nh l·ªách mapping)\n",
    "Xtr_oh, Xval_oh, ytr_oh, yval_oh = train_test_split(\n",
    "    X_train_onehot, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "Xtr_lb, Xval_lb, ytr_lb, yval_lb = train_test_split(\n",
    "    X_train_label,  y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# ===================== Models =====================\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "# ===================== Train & Evaluate =====================\n",
    "def evaluate_models(X_train, X_val, y_train, y_val, models, prefix):\n",
    "    results = {}\n",
    "\n",
    "    # chu·∫©n ho√° nh√£n y_val v·ªÅ 0/1 ƒë·ªÉ t√≠nh metric nh·∫•t qu√°n (0=e, 1=p)\n",
    "    y_true_bin = _to_binary(y_val)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "        # D·ª± ƒëo√°n nh√£n v√† x√°c su·∫•t (n·∫øu c√≥)\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_bin = _to_binary(y_pred)\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            pos_idx = _pos_index_for_proba(model)\n",
    "            y_proba = model.predict_proba(X_val)[:, pos_idx]\n",
    "        else:\n",
    "            y_proba = None\n",
    "\n",
    "        # metrics tr√™n nh·ªã ph√¢n (pos = 1 = 'p')\n",
    "        res = {\n",
    "            'Accuracy':  accuracy_score(y_true_bin, y_pred_bin),\n",
    "            'Precision': precision_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "            'Recall':    recall_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "            'F1':        f1_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "            'ROC-AUC':   roc_auc_score(y_true_bin, y_proba) if y_proba is not None else 0.0\n",
    "        }\n",
    "        results[name] = res\n",
    "\n",
    "        # Log\n",
    "        print(f\"\\n[{prefix}] {name}\")\n",
    "        for k, v in res.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "        # Confusion matrix hi·ªÉn th·ªã e/p\n",
    "        cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0, 1])\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['e', 'p'],\n",
    "                    yticklabels=['e', 'p'])\n",
    "        plt.title(f'Confusion Matrix - {prefix} - {name}')\n",
    "        plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "        plt.tight_layout()\n",
    "        fn = f'confusion_matrix_{prefix.lower().replace(\" \", \"_\")}_{name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(fn, dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "    # L∆∞u JSON k·∫øt qu·∫£ (ƒë√∫ng pattern B∆∞·ªõc 4 ƒëang ƒë·ªçc)\n",
    "    out_json = f'stream_a_{prefix.lower().replace(\" \", \"_\")}_results.json'\n",
    "    with open(out_json, 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    print(f\"\\nƒê√£ l∆∞u k·∫øt qu·∫£: {out_json}\")\n",
    "    return results\n",
    "\n",
    "print(\"=== Hu·∫•n luy·ªán & ƒê√°nh gi√°: Lu·ªìng A - One-Hot Encoding ===\")\n",
    "res_oh = evaluate_models(Xtr_oh, Xval_oh, ytr_oh, yval_oh, models, 'One-Hot Encoding')\n",
    "\n",
    "print(\"\\n=== Hu·∫•n luy·ªán & ƒê√°nh gi√°: Lu·ªìng A - Label Encoding ===\")\n",
    "res_lb = evaluate_models(Xtr_lb, Xval_lb, ytr_lb, yval_lb, models, 'Label Encoding')\n",
    "\n",
    "# ===================== So s√°nh nhanh v·ªõi Random Forest =====================\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Lu·ªìng':    ['One-Hot Encoding', 'Label Encoding'],\n",
    "    'Accuracy': [res_oh['Random Forest']['Accuracy'], res_lb['Random Forest']['Accuracy']],\n",
    "    'F1':       [res_oh['Random Forest']['F1'],       res_lb['Random Forest']['F1']]\n",
    "}).round(4)\n",
    "print(\"\\n=== B·∫£ng So s√°nh Hi·ªáu su·∫•t (Random Forest) cho Lu·ªìng A ===\")\n",
    "print(comparison_df)\n",
    "comparison_df.to_csv('model_comparison_stream_a.csv', index=False)\n",
    "\n",
    "# ===================== Submission (NH√ÉN e/p theo ƒë·ªÅ m·ªõi) =====================\n",
    "print(\"\\n=== T·∫°o file submission cho Kaggle (id, class = 'e'/'p') ===\")\n",
    "\n",
    "# üîí LU√îN ƒë·ªçc test.csv ƒë·ªÉ l·∫•y ƒë√∫ng b·ªô v√† th·ª© t·ª± id (tr√°nh l·ªách s·ªë d√≤ng)\n",
    "test_df = smart_read_csv([\n",
    "    \"/kaggle/input/mushroom-classification-btl/test.csv\",  # competition input\n",
    "    \"/kaggle/working/test.csv\",\n",
    "    \"./test.csv\",\n",
    "    \"/mnt/data/test.csv\",\n",
    "])\n",
    "if test_df is None:\n",
    "    raise FileNotFoundError(\"‚ùå Kh√¥ng t√¨m th·∫•y/ƒë·ªçc ƒë∆∞·ª£c test.csv g·ªëc.\")\n",
    "\n",
    "if 'id' in test_df.columns:\n",
    "    test_ids = test_df['id']\n",
    "elif 'Id' in test_df.columns:\n",
    "    test_ids = test_df['Id'].rename('id')\n",
    "else:\n",
    "    raise ValueError(\"‚ùå test.csv kh√¥ng c√≥ c·ªôt id/Id.\")\n",
    "\n",
    "print(f\"‚úÖ L·∫•y {len(test_ids)} h√†ng id t·ª´ test.csv\")\n",
    "\n",
    "# Train m√¥ h√¨nh cu·ªëi ƒë·ªÉ n·ªôp: Random Forest tr√™n One-Hot\n",
    "rf_onehot = RandomForestClassifier(random_state=42)\n",
    "rf_onehot.fit(X_train_onehot, y_train.values.ravel())\n",
    "\n",
    "# D·ª± ƒëo√°n NH√ÉN cho test (c√≥ th·ªÉ l√† 0/1 ho·∫∑c 'e'/'p')\n",
    "pred_any = rf_onehot.predict(X_test_onehot)\n",
    "\n",
    "# Chu·∫©n ho√° nh√£n n·ªôp v·ªÅ 'e'/'p'\n",
    "if set(pd.Series(pred_any).unique()) <= {0, 1}:\n",
    "    pred_cls = np.where(pred_any == 1, 'p', 'e')\n",
    "else:\n",
    "    pred_cls = pd.Series(pred_any).astype(str).str.lower()\n",
    "    ok = pred_cls.isin(['e','p']).all()\n",
    "    if not ok:\n",
    "        raise ValueError(\"‚ùå D·ª± ƒëo√°n kh√¥ng thu·ªôc {'e','p'} ho·∫∑c {0,1}. Ki·ªÉm tra l·∫°i pipeline/labels.\")\n",
    "\n",
    "# T·∫°o submission.csv ƒë√∫ng format (s·ªë d√≤ng = s·ªë d√≤ng test)\n",
    "submission = pd.DataFrame({'id': test_ids.values, 'class': pred_cls})\n",
    "assert len(submission) == len(test_df), f\"S·ªë d√≤ng l·ªách: submission={len(submission)} vs test={len(test_df)}\"\n",
    "\n",
    "# Ghi file (kh√¥ng c√≥ d·∫•u nh√°y k√©p)\n",
    "submission.to_csv('submission.csv', index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "print(\"‚úÖ ƒê√É T·∫†O FILE N·ªòP: submission.csv  (c·ªôt id, class = 'e'/'p')\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hu·∫•n luy·ªán cho Lu·ªìng B (Autoencoder)\n",
    "* M·ª•c ti√™u: Hu·∫•n luy·ªán v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh (Decision Tree, Random Forest, Logistic Regression, SVM, XGBoost, LightGBM) tr√™n d·ªØ li·ªáu t·ª´ Lu·ªìng B (Autoencoder features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== 3B: Lu·ªìng B ‚Äì Autoencoder (Classification, output e/p) ======\n",
    "import os, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def smart_read_csv(candidates):\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p):\n",
    "            try:\n",
    "                return pd.read_csv(p)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def _to_binary(y):\n",
    "    \"\"\"Chu·∫©n ho√° nh√£n v·ªÅ 0/1. H·ªó tr·ª£ y l√† 0/1 ho·∫∑c 'e'/'p'.\"\"\"\n",
    "    s = pd.Series(y)\n",
    "    uniq = set(s.unique())\n",
    "    if uniq <= {0, 1}:\n",
    "        return s.astype(int)\n",
    "    mapping = {'e': 0, 'p': 1, 'E': 0, 'P': 1}\n",
    "    out = s.map(mapping)\n",
    "    if out.isna().any():\n",
    "        raise ValueError(f\"Nh√£n kh√¥ng thu·ªôc {{0,1,'e','p'}}: {sorted(list(uniq))[:5]}\")\n",
    "    return out.astype(int)\n",
    "\n",
    "def _pos_index_for_proba(model):\n",
    "    \"\"\"L·∫•y index c·ªßa l·ªõp d∆∞∆°ng trong predict_proba theo model.classes_.\"\"\"\n",
    "    if hasattr(model, \"classes_\"):\n",
    "        classes = list(model.classes_)\n",
    "        if 'p' in classes:\n",
    "            return classes.index('p')\n",
    "        if 1 in classes:\n",
    "            return classes.index(1)\n",
    "        try:\n",
    "            return classes.index(max(classes))\n",
    "        except Exception:\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "# ---------- Load ----------\n",
    "X_train_auto = pd.read_csv(\"/kaggle/working/X_train_autoencoder.csv\")\n",
    "X_test_auto  = pd.read_csv(\"/kaggle/working/X_test_autoencoder.csv\")\n",
    "y_train_auto = pd.read_csv(\"/kaggle/working/y_train_autoencoder.csv\")[\"class\"]  # 'e'/'p' ho·∫∑c 0/1\n",
    "\n",
    "Xtr_a, Xval_a, ytr_a, yval_a = train_test_split(\n",
    "    X_train_auto, y_train_auto, test_size=0.2, random_state=42, stratify=y_train_auto\n",
    ")\n",
    "\n",
    "# ---------- Models ----------\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"SVM\": SVC(random_state=42, probability=True),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, eval_metric=\"mlogloss\", use_label_encoder=False),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42, verbose=-1),\n",
    "}\n",
    "\n",
    "# ---------- Train & Evaluate ----------\n",
    "def evaluate_models(X_train, X_val, y_train, y_val, models, prefix):\n",
    "    results = {}\n",
    "    y_true_bin = _to_binary(y_val)  # 0=e, 1=p\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_bin = _to_binary(y_pred)\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            pos_idx = _pos_index_for_proba(model)\n",
    "            y_proba = model.predict_proba(X_val)[:, pos_idx]\n",
    "        else:\n",
    "            y_proba = None\n",
    "\n",
    "        res = {\n",
    "            \"Accuracy\":  accuracy_score(y_true_bin, y_pred_bin),\n",
    "            \"Precision\": precision_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "            \"Recall\":    recall_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "            \"F1\":        f1_score(y_true_bin, y_pred_bin, zero_division=0),\n",
    "            \"ROC-AUC\":   roc_auc_score(y_true_bin, y_proba) if y_proba is not None else 0.0,\n",
    "        }\n",
    "        results[name] = res\n",
    "\n",
    "        print(f\"\\n[{prefix}] {name}\")\n",
    "        for k, v in res.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "        cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=[\"e\",\"p\"], yticklabels=[\"e\",\"p\"])\n",
    "        plt.title(f\"Confusion Matrix - {prefix} - {name}\")\n",
    "        plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "        plt.tight_layout()\n",
    "        fn = f'confusion_matrix_{prefix.lower().replace(\" \", \"_\")}_{name.replace(\" \", \"_\")}.png'\n",
    "        plt.savefig(fn, dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "    out_json = f'stream_b_{prefix.lower().replace(\" \", \"_\")}_results.json'\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    print(f\"\\nƒê√£ l∆∞u k·∫øt qu·∫£: {out_json}\")\n",
    "    return results\n",
    "\n",
    "print(\"=== Hu·∫•n luy·ªán & ƒê√°nh gi√° cho Lu·ªìng B - Autoencoder ===\")\n",
    "results_auto = evaluate_models(Xtr_a, Xval_a, ytr_a, yval_a, models, \"Autoencoder\")\n",
    "\n",
    "# ---------- Submission (id,class = e/p) ----------\n",
    "print(\"\\n=== D·ª± ƒëo√°n Test Set (Random Forest) & T·∫°o submission cho Kaggle ===\")\n",
    "\n",
    "# ƒê·ªçc test.csv g·ªëc ƒë·ªÉ l·∫•y id + s·ªë d√≤ng chu·∫©n\n",
    "test_df = smart_read_csv([\n",
    "    \"/kaggle/input/mushroom-classification-btl/test.csv\",\n",
    "    \"/kaggle/working/test.csv\",\n",
    "    \"./test.csv\",\n",
    "    \"/mnt/data/test.csv\",\n",
    "])\n",
    "if test_df is None:\n",
    "    raise FileNotFoundError(\"‚ùå Kh√¥ng t√¨m th·∫•y file test.csv g·ªëc.\")\n",
    "\n",
    "if \"id\" in test_df.columns:\n",
    "    test_ids = test_df[\"id\"]\n",
    "elif \"Id\" in test_df.columns:\n",
    "    test_ids = test_df[\"Id\"].rename(\"id\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå test.csv kh√¥ng c√≥ c·ªôt id/Id.\")\n",
    "\n",
    "print(f\"‚úÖ L·∫•y {len(test_ids)} h√†ng id t·ª´ test.csv\")\n",
    "\n",
    "# Train l·∫°i Random Forest tr√™n to√†n b·ªô d·ªØ li·ªáu Autoencoder\n",
    "best_model_auto = RandomForestClassifier(random_state=42)\n",
    "best_model_auto.fit(X_train_auto, y_train_auto.values.ravel())\n",
    "pred_any = best_model_auto.predict(X_test_auto)\n",
    "\n",
    "# Chu·∫©n ho√° nh√£n -> e/p\n",
    "if set(pd.Series(pred_any).unique()) <= {0,1}:\n",
    "    pred_cls = np.where(pred_any==1, \"p\", \"e\")\n",
    "else:\n",
    "    pred_cls = pd.Series(pred_any).astype(str).str.lower()\n",
    "    if not pred_cls.isin([\"e\",\"p\"]).all():\n",
    "        raise ValueError(\"‚ùå D·ª± ƒëo√°n kh√¥ng h·ª£p l·ªá, ph·∫£i thu·ªôc {'e','p'} ho·∫∑c {0,1}.\")\n",
    "\n",
    "# T·∫°o submission.csv ƒë√∫ng format\n",
    "submission = pd.DataFrame({\"id\": test_ids.values, \"class\": pred_cls})\n",
    "assert len(submission) == len(test_df), f\"S·ªë d√≤ng l·ªách: submission={len(submission)} vs test={len(test_df)}\"\n",
    "\n",
    "submission.to_csv(\"submission_auto.csv\", index=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\")\n",
    "print(\"‚úÖ ƒê√É T·∫†O FILE N·ªòP: submission_auto.csv (c·ªôt id,class = e/p)\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 4: ƒê√°nh gi√° v√† Ph√¢n t√≠ch K·∫øt qu·∫£\n",
    "* M·ª•c ti√™u:\n",
    "  1. S·ª≠ d·ª•ng c√°c ch·ªâ s·ªë ph√π h·ª£p ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh:\n",
    "     1. ƒê·ªëi v·ªõi ph√¢n lo·∫°i (edible vs poisonous): Accuracy, Precision, Recall, F1, ROC-AUC.\n",
    "     2. ƒê·ªëi v·ªõi h·ªìi quy (n·∫øu c√≥): RMSE, MAE, R¬≤ (kh√¥ng √°p d·ª•ng trong tr∆∞·ªùng h·ª£p n√†y v√¨ b√†i to√°n l√† ph√¢n lo·∫°i).\n",
    "  2. L·∫≠p b·∫£ng so s√°nh hi·ªáu su·∫•t gi·ªØa hai lu·ªìng ƒë·∫∑c tr∆∞ng (Lu·ªìng A: One-Hot Encoding v√† Label Encoding; Lu·ªìng B: Autoencoder) ƒë·ªÉ x√°c ƒë·ªãnh lu·ªìng n√†o hi·ªáu qu·∫£ h∆°n.\n",
    "  3. Ph√¢n t√≠ch k·∫øt qu·∫£ ƒë·ªÉ r√∫t ra nh·∫≠n ƒë·ªãnh v·ªÅ ch·∫•t l∆∞·ª£ng ƒë·∫∑c tr∆∞ng v√† hi·ªáu qu·∫£ c·ªßa c√°c m√¥ h√¨nh, ƒë·ªìng th·ªùi ƒë·ªÅ xu·∫•t c·∫£i ti·∫øn n·∫øu c·∫ßn (n·∫øu c√≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng nh∆∞ overfitting ho·∫∑c data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# --- C·∫•u h√¨nh t√¨m file ---\n",
    "SEARCH_DIRS = ['.', '/kaggle/working', '/mnt/data']\n",
    "\n",
    "# Map prefix h·ª£p l·ªá -> c√°c t√™n file c√≥ th·ªÉ (ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi B∆∞·ªõc 3)\n",
    "CANDIDATE_JSON_NAMES = {\n",
    "    'stream_a_one-hot_encoding': ['stream_a_one-hot_encoding_results.json'],\n",
    "    'stream_a_label_encoding':   ['stream_a_label_encoding_results.json'],\n",
    "    'stream_b_autoencoder':      ['stream_b_autoencoder_results.json'],\n",
    "}\n",
    "\n",
    "def _find_existing_file(candidates):\n",
    "    \"\"\"\n",
    "    T√¨m file trong SEARCH_DIRS theo danh s√°ch t√™n 'candidates'.\n",
    "    Tr·∫£ v·ªÅ path n·∫øu t√¨m th·∫•y, ng∆∞·ª£c l·∫°i None.\n",
    "    \"\"\"\n",
    "    for d in SEARCH_DIRS:\n",
    "        for name in candidates:\n",
    "            path = os.path.join(d, name)\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "    return None\n",
    "\n",
    "def load_results(file_or_prefix):\n",
    "    \"\"\"\n",
    "    - N·∫øu truy·ªÅn ƒë∆∞·ªùng d·∫´n .json: d√πng tr·ª±c ti·∫øp.\n",
    "    - N·∫øu truy·ªÅn prefix (v√≠ d·ª•: 'stream_a_one-hot_encoding'):\n",
    "        + Th·ª≠ c√°c t√™n file ·ª©ng v·ªõi prefix trong CANDIDATE_JSON_NAMES\n",
    "        + T√¨m trong SEARCH_DIRS\n",
    "    Tr·∫£ v·ªÅ dict {model: {Accuracy, Precision, Recall, F1, ROC-AUC}} ho·∫∑c None.\n",
    "    \"\"\"\n",
    "    # TH1: truy·ªÅn tr·ª±c ti·∫øp ƒë∆∞·ªùng d·∫´n .json\n",
    "    if isinstance(file_or_prefix, str) and file_or_prefix.endswith('.json'):\n",
    "        file_path = file_or_prefix if os.path.exists(file_or_prefix) else None\n",
    "        if file_path is None:\n",
    "            print(f\"File {file_or_prefix} kh√¥ng t·ªìn t·∫°i.\")\n",
    "            return None\n",
    "    else:\n",
    "        # TH2: truy·ªÅn prefix\n",
    "        prefix = file_or_prefix\n",
    "        # N·∫øu kh√¥ng c√≥ trong map, v·∫´n th·ª≠ gh√©p _results.json\n",
    "        candidates = CANDIDATE_JSON_NAMES.get(prefix, [f\"{prefix}_results.json\"])\n",
    "        file_path = _find_existing_file(candidates)\n",
    "        if file_path is None:\n",
    "            tried = [os.path.join(d, n) for d in SEARCH_DIRS for n in candidates]\n",
    "            print(\"Kh√¥ng t√¨m th·∫•y file k·∫øt qu·∫£ cho prefix:\", prefix)\n",
    "            print(\"ƒê√£ th·ª≠ c√°c ƒë∆∞·ªùng d·∫´n sau:\")\n",
    "            for t in tried:\n",
    "                print(\"  -\", t)\n",
    "            return None\n",
    "\n",
    "    # ƒê·ªçc JSON\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"L·ªói gi·∫£i m√£ JSON trong file {file_path}. H√£y ki·ªÉm tra ho·∫∑c ch·∫°y l·∫°i hu·∫•n luy·ªán.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi m·ªü {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Chu·∫©n h√≥a c·∫•u tr√∫c\n",
    "    if isinstance(data, dict) and all(isinstance(v, dict) for v in data.values()):\n",
    "        return data\n",
    "    elif isinstance(data, dict):\n",
    "        results = {}\n",
    "        for model, metrics in data.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                results[model] = metrics\n",
    "            else:\n",
    "                results[model] = {'Accuracy': 0, 'Precision': 0, 'Recall': 0, 'F1': 0, 'ROC-AUC': 0}\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"C·∫•u tr√∫c file {file_path} kh√¥ng h·ª£p l·ªá.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ====== T·∫¢I K·∫æT QU·∫¢ (d√πng ƒë√∫ng prefix) ======\n",
    "results_stream_a_onehot = load_results('stream_a_one-hot_encoding')\n",
    "results_stream_a_label  = load_results('stream_a_label_encoding')\n",
    "results_stream_b_auto   = load_results('stream_b_autoencoder')\n",
    "\n",
    "# Ki·ªÉm tra thi·∫øu\n",
    "missing = []\n",
    "if results_stream_a_onehot is None:\n",
    "    missing.append('stream_a_one-hot_encoding_results.json')\n",
    "if results_stream_a_label is None:\n",
    "    missing.append('stream_a_label_encoding_results.json')\n",
    "if results_stream_b_auto is None:\n",
    "    missing.append('stream_b_autoencoder_results.json')\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nL·ªói: Thi·∫øu file JSON k·∫øt qu·∫£ sau:\")\n",
    "    for m in missing:\n",
    "        print(\"  -\", m)\n",
    "    print(\"\\nG·ª£i √Ω kh·∫Øc ph·ª•c:\")\n",
    "    print(\"- ƒê·∫£m b·∫£o ƒë√£ CH·∫†Y xong B∆∞·ªõc 3 v√† c√°c file tr√™n ƒë√£ ƒë∆∞·ª£c t·∫°o.\")\n",
    "    print(\"- N·∫øu b·∫°n kh√¥ng ch·∫°y tr√™n Kaggle, h√£y ki·ªÉm tra th∆∞ m·ª•c hi·ªán t·∫°i ho·∫∑c /mnt/data.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ====== T·∫†O B·∫¢NG SO S√ÅNH ======\n",
    "def create_comparison_df(results, prefix):\n",
    "    rows = []\n",
    "    for model, metrics in results.items():\n",
    "        rows.append({\n",
    "            'Lu·ªìng': prefix,\n",
    "            'M√¥ h√¨nh': model,\n",
    "            'Accuracy': metrics.get('Accuracy', 0),\n",
    "            'Precision': metrics.get('Precision', 0),\n",
    "            'Recall': metrics.get('Recall', 0),\n",
    "            'F1': metrics.get('F1', 0),\n",
    "            'ROC-AUC': metrics.get('ROC-AUC', 0)\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_stream_a_onehot = create_comparison_df(results_stream_a_onehot, 'Lu·ªìng A - One-Hot Encoding')\n",
    "df_stream_a_label  = create_comparison_df(results_stream_a_label, 'Lu·ªìng A - Label Encoding')\n",
    "df_stream_b_auto   = create_comparison_df(results_stream_b_auto,  'Lu·ªìng B - Autoencoder')\n",
    "\n",
    "combined_df = pd.concat([df_stream_a_onehot, df_stream_a_label, df_stream_b_auto], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== B·∫£ng So s√°nh Hi·ªáu su·∫•t Chi ti·∫øt gi·ªØa Lu·ªìng A v√† Lu·ªìng B ===\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(combined_df.round(4))\n",
    "pd.reset_option('display.max_columns')\n",
    "\n",
    "# L∆∞u\n",
    "out_csv = 'combined_model_comparison_detailed.csv'\n",
    "combined_df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nƒê√£ l∆∞u b·∫£ng t·ªïng h·ª£p: {out_csv}\")\n",
    "\n",
    "# ====== V·∫º BI·ªÇU ƒê·ªí F1 ======\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='M√¥ h√¨nh', y='F1', hue='Lu·ªìng', data=combined_df, palette='viridis')\n",
    "plt.title('So s√°nh F1 Score gi·ªØa Lu·ªìng A v√† Lu·ªìng B theo M√¥ h√¨nh')\n",
    "plt.xlabel('M√¥ h√¨nh')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Lu·ªìng')\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_comparison_plot.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ====== PH√ÇN T√çCH ======\n",
    "print(\"\\n=== Ph√¢n t√≠ch K·∫øt qu·∫£ ===\")\n",
    "print(\"1) N·∫øu c√°c ch·ªâ s·ªë ƒë·ªÅu ~1.0, nhi·ªÅu kh·∫£ nƒÉng d·ªØ li·ªáu x√°c th·ª±c qu√° d·ªÖ ho·∫∑c c√≥ r√≤ r·ªâ d·ªØ li·ªáu (data leakage).\")\n",
    "print(\"2) Ki·ªÉm tra c√°c ·∫£nh confusion_matrix_*.png ƒë·ªÉ x√°c th·ª±c ph√¢n ph·ªëi d·ª± ƒëo√°n.\")\n",
    "print(\"3) N·∫øu nghi ng·ªù overfitting ho·∫∑c leakage, th·ª≠ ƒë·ªïi random_state/c√°ch chia data ho·∫∑c d√πng cross-validation.\")\n",
    "print(\"4) Ch·ªçn m√¥ h√¨nh/lu·ªìng c√≥ F1 cao nh·∫•t cho suy di·ªÖn test (test_predictions_*.csv).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 5: So s√°nh v√† K·∫øt lu·∫≠n\n",
    "\n",
    "üìå **L∆∞u √Ω c·ªßa ƒë·ªÅ b√†i**  \n",
    "- Ch·ªâ s·ª≠ d·ª•ng **d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p** (kh√¥ng d√πng external data).  \n",
    "- Sinh vi√™n ph·∫£i m√¥ t·∫£ r√µ r√†ng c√°c b∆∞·ªõc x·ª≠ l√Ω, l·ª±a ch·ªçn thu·∫≠t to√°n, tham s·ªë.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Ph√¢n t√≠ch Lu·ªìng A ‚Äì K·ªπ thu·∫≠t truy·ªÅn th·ªëng (One-Hot & Label Encoding)\n",
    "\n",
    "**∆Øu ƒëi·ªÉm**\n",
    "- **ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu, d·ªÖ tri·ªÉn khai**  \n",
    "  ‚Üí V√¨ ch·ªâ d√πng c√°c k·ªπ thu·∫≠t c∆° b·∫£n trong sklearn (OneHotEncoder, LabelEncoder), kh√¥ng c·∫ßn m√¥ h√¨nh ph·ª©c t·∫°p hay GPU.  \n",
    "- **K·∫øt qu·∫£ minh b·∫°ch, d·ªÖ gi·∫£i th√≠ch**  \n",
    "  ‚Üí M·ªói c·ªôt one-hot th·ªÉ hi·ªán r√µ m·ªôt gi√° tr·ªã c·ªßa ƒë·∫∑c tr∆∞ng (v√≠ d·ª•: ‚Äúodor = foul‚Äù), n√™n d·ªÖ li√™n h·ªá tr·ª±c ti·∫øp ƒë·∫øn √Ω nghƒ©a th·ª±c t·∫ø.  \n",
    "- **Hi·ªáu qu·∫£ cho d·ªØ li·ªáu ph√¢n lo·∫°i m·∫°nh**  \n",
    "  ‚Üí V·ªõi b√†i to√°n n·∫•m, c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i nh∆∞ `odor`, `spore-print-color`, `gill-color` v·ªën t√°ch bi·ªát r√µ edible/poisonous, n√™n one-hot ƒë√£ ƒë·ªß m·∫°nh ƒë·ªÉ ph√¢n lo·∫°i g·∫ßn nh∆∞ ho√†n h·∫£o.  \n",
    "- **H·ªó tr·ª£ feature engineering th·ªß c√¥ng**  \n",
    "  ‚Üí Ng∆∞·ªùi d√πng c√≥ th·ªÉ ch·ªß ƒë·ªông t·∫°o ƒë·∫∑c tr∆∞ng m·ªõi (interaction features) d·ª±a tr√™n ki·∫øn th·ª©c mi·ªÅn, ƒëi·ªÅu m√† embedding t·ª± ƒë·ªông kh√≥ ƒë·∫£m b·∫£o.\n",
    "\n",
    "**Nh∆∞·ª£c ƒëi·ªÉm**\n",
    "- **One-Hot Encoding l√†m tƒÉng s·ªë chi·ªÅu d·ªØ li·ªáu**  \n",
    "  ‚Üí V√¨ m·ªói gi√° tr·ªã c·ªßa ƒë·∫∑c tr∆∞ng l·∫°i sinh ra m·ªôt c·ªôt ri√™ng, d·∫´n ƒë·∫øn ma tr·∫≠n th∆∞a v√† t·ªën RAM.  \n",
    "- **Label Encoding d·ªÖ g√¢y sai l·ªách**  \n",
    "  ‚Üí V√¨ g√°n s·ªë nguy√™n (0,1,2,...) cho c√°c gi√° tr·ªã ph√¢n lo·∫°i, t·∫°o ra quan h·ªá th·ª© t·ª± gi·∫£ m·∫°o m√† m√¥ h√¨nh tuy·∫øn t√≠nh c√≥ th·ªÉ hi·ªÉu nh·∫ßm.  \n",
    "- **Kh·∫£ nƒÉng bi·ªÉu di·ªÖn h·∫°n ch·∫ø**  \n",
    "  ‚Üí V√¨ ch·ªâ ph·∫£n √°nh m·ªëi quan h·ªá tuy·∫øn t√≠nh ƒë∆°n gi·∫£n, kh√≥ m√¥ h√¨nh h√≥a c√°c ph·ª• thu·ªôc ph·ª©c t·∫°p ho·∫∑c phi tuy·∫øn gi·ªØa nhi·ªÅu ƒë·∫∑c tr∆∞ng.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Ph√¢n t√≠ch Lu·ªìng B ‚Äì Autoencoder\n",
    "\n",
    "**∆Øu ƒëi·ªÉm**\n",
    "- **Gi·∫£m chi·ªÅu d·ªØ li·ªáu**  \n",
    "  ‚Üí Autoencoder n√©n d·ªØ li·ªáu th√†nh vector nh·ªè g·ªçn (bottleneck), gi√∫p c√°c m√¥ h√¨nh ph√≠a sau hu·∫•n luy·ªán nhanh h∆°n v√† tr√°nh curse of dimensionality.  \n",
    "- **H·ªçc ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng phi tuy·∫øn ph·ª©c t·∫°p**  \n",
    "  ‚Üí Nh·ªù m·∫°ng n∆°-ron nhi·ªÅu l·ªõp, autoencoder c√≥ th·ªÉ kh√°m ph√° m·ªëi quan h·ªá ·∫©n gi·ªØa c√°c ƒë·∫∑c tr∆∞ng, ƒëi·ªÅu m√† one-hot kh√¥ng th·ªÉ.  \n",
    "- **Embedding c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng**  \n",
    "  ‚Üí Vector bottleneck c√≥ th·ªÉ d√πng l√†m ƒë·∫ßu v√†o cho nhi·ªÅu m√¥ h√¨nh kh√°c nhau, gi·ªëng m·ªôt d·∫°ng ‚Äúfeature representation‚Äù chung.\n",
    "\n",
    "**Nh∆∞·ª£c ƒëi·ªÉm**\n",
    "- **Kh√≥ gi·∫£i th√≠ch (m·∫•t interpretability)**  \n",
    "  ‚Üí V√¨ m·ªói chi·ªÅu trong vector embedding kh√¥ng c√≤n g·∫Øn v·ªõi m·ªôt ƒë·∫∑c tr∆∞ng g·ªëc, n√™n kh√≥ tr·∫£ l·ªùi ‚Äúƒë·∫∑c tr∆∞ng n√†o ·∫£nh h∆∞·ªüng nhi·ªÅu nh·∫•t‚Äù.  \n",
    "- **Chi ph√≠ hu·∫•n luy·ªán cao**  \n",
    "  ‚Üí C·∫ßn nhi·ªÅu epoch, tuning si√™u tham s·ªë v√† th∆∞·ªùng ph·∫£i ch·∫°y tr√™n GPU, ph·ª©c t·∫°p h∆°n nhi·ªÅu so v·ªõi encoding truy·ªÅn th·ªëng.  \n",
    "- **Nh·∫°y c·∫£m v·ªõi tham s·ªë**  \n",
    "  ‚Üí C√°c y·∫øu t·ªë nh∆∞ s·ªë t·∫ßng, s·ªë neuron, learning rate‚Ä¶ ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn ch·∫•t l∆∞·ª£ng embedding. N·∫øu tuning kh√¥ng t·ªët d·ªÖ d·∫´n ƒë·∫øn underfitting ho·∫∑c overfitting.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ K·∫øt lu·∫≠n v√† Khuy·∫øn ngh·ªã\n",
    "\n",
    "1. **K·∫øt qu·∫£ th·ª±c nghi·ªám**:  \n",
    "   - Lu·ªìng A (ƒë·∫∑c bi·ªát One-Hot Encoding) ƒë·∫°t Accuracy/F1 ‚âà 1.0 ‚Üí m√¥ h√¨nh ph√¢n lo·∫°i g·∫ßn nh∆∞ ho√†n h·∫£o.  \n",
    "   - Lu·ªìng B c≈©ng cho k·∫øt qu·∫£ t·ªët, nh∆∞ng kh√¥ng v∆∞·ª£t tr·ªôi, trong khi chi ph√≠ v√† ƒë·ªô ph·ª©c t·∫°p cao h∆°n.  \n",
    "\n",
    "2. **So s√°nh t·ªïng th·ªÉ**:  \n",
    "   - B√†i to√°n n·∫•m v·ªën c√≥ ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i m·∫°nh v√† t√°ch bi·ªát r√µ r√†ng ‚Üí one-hot encoding ƒë·ªß ƒë·ªÉ khai th√°c tri·ªát ƒë·ªÉ th√¥ng tin.  \n",
    "   - Autoencoder tuy hi·ªán ƒë·∫°i nh∆∞ng kh√¥ng mang l·∫°i l·ª£i √≠ch r√µ r·ªát trong b·ªëi c·∫£nh n√†y.  \n",
    "\n",
    "3. **Khuy·∫øn ngh·ªã cu·ªëi c√πng**:  \n",
    "   - Ch·ªçn **Lu·ªìng A ‚Äì One-Hot Encoding** cho tri·ªÉn khai th·ª±c t·∫ø: v√¨ ƒë·∫°t hi·ªáu su·∫•t cao, d·ªÖ hi·ªÉu, d·ªÖ √°p d·ª•ng v√† √≠t t·ªën t√†i nguy√™n.  \n",
    "   - Gi·ªØ **Lu·ªìng B ‚Äì Autoencoder** l√†m h∆∞·ªõng nghi√™n c·ª©u m·ªü r·ªông: khi d·ªØ li·ªáu tr·ªü n√™n l·ªõn h∆°n, nhi·ªÅu ƒë·∫∑c tr∆∞ng h∆°n, ho·∫∑c c·∫ßn gi·∫£m chi·ªÅu ƒë·ªÉ k·∫øt h·ª£p v·ªõi m√¥ h√¨nh ph·ª©c t·∫°p.  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Gi·∫£i th√≠ch ng·∫Øn g·ªçn\n",
    "Autoencoder c√≥ ∆∞u th·∫ø trong vi·ªác **n√©n th√¥ng tin** v√† kh√°m ph√° quan h·ªá ph·ª©c t·∫°p, nh∆∞ng ƒë√°nh ƒë·ªïi b·∫±ng **m·∫•t t√≠nh di·ªÖn gi·∫£i v√† chi ph√≠ cao**. Trong khi ƒë√≥, One-Hot Encoding v·ª´a **ƒë∆°n gi·∫£n, minh b·∫°ch, hi·ªáu qu·∫£**, l·∫°i cho k·∫øt qu·∫£ g·∫ßn nh∆∞ ho√†n h·∫£o trong b√†i to√°n ph√¢n lo·∫°i n·∫•m. V√¨ th·∫ø, l·ª±a ch·ªçn h·ª£p l√Ω nh·∫•t cho b√†i to√°n n√†y l√† **Lu·ªìng A**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ sklearn version: 1.6.1\n",
      "Accuracy: 1.0\n",
      "F1 (pos='p'): 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           e     1.0000    1.0000    1.0000       631\n",
      "           p     1.0000    1.0000    1.0000       588\n",
      "\n",
      "    accuracy                         1.0000      1219\n",
      "   macro avg     1.0000    1.0000    1.0000      1219\n",
      "weighted avg     1.0000    1.0000    1.0000      1219\n",
      "\n",
      "üíæ Saved: C:\\Users\\Duy Ti·∫øn\\Desktop\\btl\\mushroom_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "# train_and_save.py\n",
    "import joblib, pandas as pd\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "TRAIN_CSV = \"Dataset\\\\train.csv\"                             # s·ª≠a ƒë∆∞·ªùng d·∫´n n·∫øu c·∫ßn\n",
    "OUT_PATH  = Path(\"mushroom_pipeline.joblib\")    # app s·∫Ω load file n√†y\n",
    "\n",
    "def main():\n",
    "    print(\"‚úÖ sklearn version:\", sklearn.__version__)  # ph·∫£i l√† 1.6.1\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "    assert \"class\" in df.columns, \"train.csv c·∫ßn c·ªôt 'class' (e/p).\"\n",
    "    y = df[\"class\"].astype(str).str.lower()          # 'e'/'p'\n",
    "    X = df.drop(columns=[\"class\"])\n",
    "\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "            # c√≥ th·ªÉ th√™m x·ª≠ l√Ω s·ªë n·∫øu c·∫ßn, v√≠ d·ª• StandardScaler cho num_cols\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "    pipe = Pipeline([(\"prep\", pre), (\"clf\", clf)])\n",
    "\n",
    "    # Train/val ƒë·ªÉ xem ch·∫•t l∆∞·ª£ng\n",
    "    Xtr, Xval, ytr, yval = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    pred = pipe.predict(Xval)\n",
    "    print(\"Accuracy:\", accuracy_score(yval, pred))\n",
    "    print(\"F1 (pos='p'):\", f1_score(yval, pred, pos_label=\"p\"))\n",
    "    print(classification_report(yval, pred, digits=4))\n",
    "\n",
    "    # Fit to√†n b·ªô & l∆∞u\n",
    "    pipe.fit(X, y)\n",
    "    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(\n",
    "        {\n",
    "            \"pipeline\": pipe,\n",
    "            \"feature_cols\": X.columns.tolist(),\n",
    "            \"cat_cols\": cat_cols,\n",
    "            \"num_cols\": num_cols,\n",
    "            \"target_labels\": [\"e\", \"p\"],\n",
    "        },\n",
    "        OUT_PATH\n",
    "    )\n",
    "    print(\"üíæ Saved:\", OUT_PATH.resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13804480,
     "isSourceIdPinned": false,
     "sourceId": 115654,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
